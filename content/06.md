#Advanced Process Management
##Process Scheduling
The process scheduler is the kernel subsystem that divides the finite resource of processor time among a system's processes. Multitasking operating systems come into two variants: cooperative and preemptive. The preemptive scheduler decides when one process is to stop running and a different process is to resume. In cooperative multitasking, conversely, a process does not stop running until it voluntarily decides to do so.

###Timeslices
The timeslice that the process scheduler allots to each process is an important variable in the overall behavior and performance of a system.

###I/O- Versus Processor-Bound Processes
Processes that continually consume all of their available timeslices are considered processor-bound. Such processes are hungry for CPU time and will consume all that the scheduler gives them. On the other hand, processes that spend more time blocked waiting for some resource than executing are considered I/O-bound. I/O-bound processes are often issuing and waiting for file or network I/O, blocking on keyboard input, or waiting for the user to move the mouse.

Processor- and I/O-bound applications differ in the type of scheduler behavior from which they most benefit. Process-bound applications crave the largest timeslices possible, allowing them to maximize cache hit rates and get their jobs done as quickly as possible. In contrast, I/O-bound processes do not necessarily need large timeslices because they typically run for only very short periods before issuing I/O requests and blocking on some kernel resource.

##The Completely Fair Scheduler
CFS introduces a quite different algorithm called fair sheduling that eliminates timeslices as the unit of allotting access to the processor. Instead of timeslices, CFS assigns each process a proportioin of the process's time. The algorithm is simple: CFS starts by assigning N processes each 1/N of the processor's time. CFS then adjusts this allotment by weighting each process's proportion by its nice value. Processes with the default nice value of zero have a weight of one, so their proportion is unchanged. Processes with a smaller nice value (higher priority) receive a larger weight, increasing their fraction of the processor, while process's with a larger nice value(lower priority) receive a smaller weight, decreasing their fraction of the processor.

CFS now has a weighted proption of processor time assigned to each process. To determine the actual length of time each process runs, CFS needs to divide the proportions into a fixed period. That period is called the target latency. Due to the cost of context switching from one process to another, known as switching costs, and the reduced temporal locality, the system's overall throughput would suffer. To deal with this situation, CFS introduces a second key variable, the minimum granularity. The minimum granularity is a floor on the length of time any process is run.

##Yielding the Processor
```C
#include <sched.h>

int sched_yield(void);
```

A call to `sched_yield()` results in suspention of the currently running process, after which the process scheduler selects a new process to run, in the same manner as if the kernel had itself preempted the currently running process in favor of executing a new process.

##Process Priorities
Unix has historically called these priorities nice values because the idea behind them was to "be nice" to other processes on the system by lowering a process's priority, allowing other processes to consume more of the system's processor time.

Legal nice values range from -20 to 19 inclusive, with a default value of 0.

###nice()
```C
#include <unistd.h>

int nice(int inc);
``` 
A successful call to `nice()` increments a process's nice value by inc and returns the newly updated value. Only a process with the CAP_SYS_NICE capability(effectively, processes owned by root) may provide a negative value for inc, decreasing its nice value and thereby increasing its priority. Consequently, nonroot processes may only lower their priority.

###getpriority() and setpriority()
```C
#include <sys/time.h>
#include <sys/resource.h>

int getpriority(int which, int who);
int setpriority(int which, int who, int prio);
```
These calls operate on the process, process group, or user, as specified by which and who. The value of which must be one of `PRIO_PROCESS`, `PRIO_PGRP`, or `PRIO_USER`, in which case who specifies a process ID, process group ID, or user ID, respectively. If who is 0, the call operates on the current process ID, process group ID, or user ID, respectively.

A call to `getpriority()` returns the highest priority of any of the specified processes. A call to `setpriority()` sets the priority of all specified processes to prio.

###I/O Priorities
In addition to a scheduling priority, Linux allows processes to specify an I/O priority. This value affects the relative priority of the processes' I/O requests. By default, I/O schedulers use a process's nice value to determine the I/O priority. However, the Linux kernel additionally provides two system calls for explicitly setting and retrieving the I/O priority independently of the nice value:

```C
int ioprio_get(int which, int who);
int ioprio_set(int which, int who, int ioprio);
```

##Processor Affinity
On a multiprocessing machine, the process scheduler must decide which processes run on each CPU. Two challenges derive from this responsibility: the scheduler must work toward fully utilizing all of the system's processor because it is inefficient for one CPU to sit idle while a process is waiting to run. However, once a process has been scheduled on one CPU, the process scheduler should aim to schedule it on the same CPU in the future. This is beneficial because migrating a process from one processor to another has costs. When a process moves from one processor to another, there are thus two associated costs: cached data is no longer accessible to the process that moved, and data in the original processor's cache must be invalidated.

The process scheduler's two goals, of course, are potencially conflicting. If one processor has a significantly larger process load than another, it makes sense to reschedule some processes on the less-busy CPU. Deciding when to move processes in reponse to such imbalances, called load balancing, is of great importance to the performance of SMP machines.

Process affinity refers to the likelihood of a process to be scheduled consistently on the same processor. The term soft affinity refers to the scheduler's natural propensity to continue scheduling a process on the same process.

Sometimes, however, the user or the application wants to be enforce a process-to-processor bond. This is often because the process is strongly cache-sensitive and desires to remain on the same processor. Bonding a process to a particular processor and having the kernel enforce the relationship is called setting a hard affinity.

###Sched_getaffinity() and sched_setaffnity()
```C
#define _GNU_SOURCE
#include <sched.h>

typedef struct cpu_set_t;

size_t CPU_SETSIZE;

void CPU_SET(unsigned long cpu, cpu_set_t *set);
void CPU_CLR(unsigned long cpu, cpu_set_t *set);
int CPU_ISSET(unsigned long cpu, cpu_set_t *set);
void CPU_ZERO(cpu_set_t *set);
int sched_setaffinity(pid_t pid, size_t setsize,
					  const cpu_set_t *set);
int sched_getaffinity(pid_t pid, size_t setsize,
					  cpu_set_t *set);
```
A call to `sched_getaffinity()` retrieves the CPU affinity of the process pid and stores it in the special cpu_set_t type, which is accessed via special macros. If pid is 0, the call retrieves the current process's affinity. The setsize parameter is the size of the cpu_set_t type, which must be used by glibc for compatilibity with future changes in the size of this type.

```C
cpu_set_t set;
int ret, i;

CPU_ZERO(&set);
ret = sched_getaffinity(0, sizeof cpu_set_t, &set);
if (ret == -1)
	perror("sched_getaffinity");

for (i = 0; i < CPU_SETSIZE; ++i) {
	int cpu;
	
	cpu = CPU_ISSET(i, &set);
	printf("cpu=%i is %s\n", i,
			cpu ? "set" : "unset");
}
```

```C
// ensure that our process runs only on CPU #0, and never on #1

cpu_set_t set;
int ret;

CPU_ZERO(&set);
CPU_SET(0, &set);
CPU_CLR(1, &set);
ret = sched_setaffinity(0, sizeof(cpu_set_t), &set);
```

##Real-Time Systems
###Hard Versus Soft Real-Time Systems
A hard real-time system requires absolute adherence to operational deadlines. Exceeding the deadlines constitutes failure and is a major bug. A soft real-time system, on the other hand, does not consider overrunning a deadline to be a critical failure.

###Linux Scheduling Policies and Priorities
The behavior of the Linux scheduler with respect to a process depends on the process's scheduling policy(SCHED_FIFO, SCHED_RR, and SCHED_OTHER).

####The first in, first out policy
The first in, first out(FIFO) class is a very simple real-time policy without timeslices. A FIFO-classed process will continue running so long as no higher-priority process becomes runnable.

####The round-robin policy
The round-robin(RR) class is identical to the FIFO class, except that it imposes additional rules in the case of processes with the same priority. It additionally ceases execution when it exhausts its timeslice, at which time it moves to the end of the list of runnable processes at its priority.

####The normal policy
SCHED_OTHER represents the standard scheduling policy, the default nonreal-time class. All normal-classed processes have a static priority of 0. Consequently, any runnable FIFO- and RR-classed process will preempt a running normal-classed process.

####The batch scheduling policy
SCHED_BATCH is the batch or idle scheduling policy. Its behavior is somewhat the antithesis of the real-time policies: processes in this class run only when there are no other runnable processes on the system, even if the other processes have exhausted their timeslices.

####Setting the Linux scheduling policy
```C
#include <sched.h>

struct sched_param {
	/* ... */
	int sched_priority;
	/* ... */
};

int sched_getscheduler(pid_t pid);
int sched_setscheduler(pid_t pid, int policy, const struct sched_param *sp);
```

###Setting scheduling Parameters
```C
#include <sched.h>

struct sched_param {
	/* ... */
	int sched_priority;
	/* ... */
};

int sched_getparam(pid_t pid, struct sched_param *sp);
int sched_setparam(pid_t pid, const struct sched_param *sp);
```

####Determining the range of valid priorities
Linux provides two system calls for retrieving the range of valid priority values. One returns the minimum value and the other returns the maximum:

```C
#include <sched.h>

int sched_get_priority_min(int policy);
int sched_get_priority_max(int policy);
```

###sched_rr_get_interval()
POSIX defines an interface for retrieving the length of a given process's timeslice:

```C
#include <sched.h>

struct timespec {
	time_t tv_sec; /* seconds */
	long tv_nsec; /* nanoseconds */
};
```

###Precautions with Real-Time Processes

- Keep in mind that any CPU-bound loop will run until completion, without interruption, if there is no higher-priority real-time process on the system. If the loop is infinite, the system will become unresponsive.
- Because real-time processes run at the expense of everything else on the system, special attention must be paid to their design. Take care not to starve the rest of the system of processor time.
- Be very careful with busy waiting. If a real-time process busy-waits for a resource held by a lower-priority process, the real-time process will busy-wait forever.
- While developing a real-time process, keep a terminal open, running as a real-time process with a higher priority than the process in development. In an emergency, the terminal will remain responsive and allow you to kill the runaway real-time process.
- The chrt utility, part of the util-linux package of tools, makes it easy to retrieve and set the real-time attributes of other processes.

##Resource Limits
```C
#include <sys/time.h>
#include <sys/resource.h>

struct rlimit {
	rlimit_t rlimit_cur; /* soft limit */
	rlimit_t rlimit_max; /* hard limit */
};

int getrlimit(int resource, struct rlimit *rlim);
int setrlimit(int resource, const struct rlimit *rlim);

// RLIMIT_AS : maximum size of a process's address space, in bytes
// RLIMIT_CORE : maximum size of core files, in bytes
// RLIMIT_CPU : maximum CPU time that a process can consume, in seconds
// RLIMIT_DATA : maximum size of a process's data segment and heap, in bytes
// RLIMIT_FSIZE : maximum file size that a process may create, in bytes
// RLIMIT_LOCKS : maximum number of file locks that a process may hold
// RLIMIT_MEMLOCK : maximum number of bytes of memory that a process without the CAP_SYS_IPC capability can lock into memory via mlock(), mlockall(), or shmctl().
// RLIMIT_MSGQUEUE : maximum number of bytes that a user may allocate for POSIX message queues
// RLIMIT_NICE : maximum value to which a process can lower its nice value
// RLIMIT_NOFILE : greater than the maximum number of fd that a process may hold open
// RLIMIT_NPROC : maximum number of processes that the user may have running on the system at any given moment
// RLIMIT_RSS : maximum number of pages that a process may have resident in memory
// RLIMIT_RTTIME : a limit on CPU time that a real-time process may consume without issuing a blocking system call
// RLIMIT_RTPRIO : maximum real-time priority level a process without the CAP_SYS_NICE capability may request
// RLIMIT_SIGPENDING : maximum number of signals that may be queued for this user
// RLIMIT_STACK : maximum size of a process's stack, in bytes.
```