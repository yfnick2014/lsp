#Advanced Process Management
##Process Scheduling
The process scheduler is the kernel subsystem that divides the finite resource of processor time among a system's processes. Multitasking operating systems come into two variants: cooperative and preemptive. The preemptive scheduler decides when one process is to stop running and a different process is to resume. In cooperative multitasking, conversely, a process does not stop running until it voluntarily decides to do so.

###Timeslices
The timeslice that the process scheduler allots to each process is an important variable in the overall behavior and performance of a system.

###I/O- Versus Processor-Bound Processes
Processes that continually consume all of their available timeslices are considered processor-bound. Such processes are hungry for CPU time and will consume all that the scheduler gives them. On the other hand, processes that spend more time blocked waiting for some resource than executing are considered I/O-bound. I/O-bound processes are often issuing and waiting for file or network I/O, blocking on keyboard input, or waiting for the user to move the mouse.

Processor- and I/O-bound applications differ in the type of scheduler behavior from which they most benefit. Process-bound applications crave the largest timeslices possible, allowing them to maximize cache hit rates and get their jobs done as quickly as possible. In contrast, I/O-bound processes do not necessarily need large timeslices because they typically run for only very short periods before issuing I/O requests and blocking on some kernel resource.

##The Completely Fair Scheduler
CFS introduces a quite different algorithm called fair sheduling that eliminates timeslices as the unit of allotting access to the processor. Instead of timeslices, CFS assigns each process a proportioin of the process's time. The algorithm is simple: CFS starts by assigning N processes each 1/N of the processor's time. CFS then adjusts this allotment by weighting each process's proportion by its nice value. Processes with the default nice value of zero have a weight of one, so their proportion is unchanged. Processes with a smaller nice value (higher priority) receive a larger weight, increasing their fraction of the processor, while process's with a larger nice value(lower priority) receive a smaller weight, decreasing their fraction of the processor.

CFS now has a weighted proption of processor time assigned to each process. To determine the actual length of time each process runs, CFS needs to divide the proportions into a fixed period. That period is called the target latency. Due to the cost of context switching from one process to another, known as switching costs, and the reduced temporal locality, the system's overall throughput would suffer. To deal with this situation, CFS introduces a second key variable, the minimum granularity. The minimum granularity is a floor on the length of time any process is run.

##Yielding the Processor
```C
#include <sched.h>

int sched_yield(void);
```

A call to `sched_yield()` results in suspention of the currently running process, after which the process scheduler selects a new process to run, in the same manner as if the kernel had itself preempted the currently running process in favor of executing a new process.

##Process Priorities
Unix has historically called these priorities nice values because the idea behind them was to "be nice" to other processes on the system by lowering a process's priority, allowing other processes to consume more of the system's processor time.

Legal nice values range from -20 to 19 inclusive, with a default value of 0.

###nice()
```C
#include <unistd.h>

int nice(int inc);
``` 
A successful call to `nice()` increments a process's nice value by inc and returns the newly updated value. Only a process with the CAP_SYS_NICE capability(effectively, processes owned by root) may provide a negative value for inc, decreasing its nice value and thereby increasing its priority. Consequently, nonroot processes may only lower their priority.

###getpriority() and setpriority()
```C
#include <sys/time.h>
#include <sys/resource.h>

int getpriority(int which, int who);
int setpriority(int which, int who, int prio);
```
These calls operate on the process, process group, or user, as specified by which and who. The value of which must be one of `PRIO_PROCESS`, `PRIO_PGRP`, or `PRIO_USER`, in which case who specifies a process ID, process group ID, or user ID, respectively. If who is 0, the call operates on the current process ID, process group ID, or user ID, respectively.

A call to `getpriority()` returns the highest priority of any of the specified processes. A call to `setpriority()` sets the priority of all specified processes to prio.

###I/O Priorities
In addition to a scheduling priority, Linux allows processes to specify an I/O priority. This value affects the relative priority of the processes' I/O requests. By default, I/O schedulers use a process's nice value to determine the I/O priority. However, the Linux kernel additionally provides two system calls for explicitly setting and retrieving the I/O priority independently of the nice value:

```C
int ioprio_get(int which, int who);
int ioprio_set(int which, int who, int ioprio);
```

##Processor Affinity
On a multiprocessing machine, the process scheduler must decide which processes run on each CPU. Two challenges derive from this responsibility: the scheduler must work toward fully utilizing all of the system's processor because it is inefficient for one CPU to sit idle while a process is waiting to run. However, once a process has been scheduled on one CPU, the process scheduler should aim to schedule it on the same CPU in the future. This is beneficial because migrating a process from one processor to another has costs. When a process moves from one processor to another, there are thus two associated costs: cached data is no longer accessible to the process that moved, and data in the original processor's cache must be invalidated.

The process scheduler's two goals, of course, are potencially conflicting. If one processor has a significantly larger process load than another, it makes sense to reschedule some processes on the less-busy CPU. Deciding when to move processes in reponse to such imbalances, called load balancing, is of great importance to the performance of SMP machines.

Process affinity refers to the likelihood of a process to be scheduled consistently on the same processor. The term soft affinity refers to the scheduler's natural propensity to continue scheduling a process on the same process.

Sometimes, however, the user or the application wants to be enforce a process-to-processor bond. This is often because the process is strongly cache-sensitive and desires to remain on the same processor. Bonding a process to a particular processor and having the kernel enforce the relationship is called setting a hard affinity.

###Sched_getaffinity() and sched_setaffnity()
```C
#define _GNU_SOURCE
#include <sched.h>

typedef struct cpu_set_t;

size_t CPU_SETSIZE;

void CPU_SET(unsigned long cpu, cpu_set_t *set);
void CPU_CLR(unsigned long cpu, cpu_set_t *set);
int CPU_ISSET(unsigned long cpu, cpu_set_t *set);
void CPU_ZERO(cpu_set_t *set);
int sched_setaffinity(pid_t pid, size_t setsize,
					  const cpu_set_t *set);
int sched_getaffinity(pid_t pid, size_t setsize,
					  cpu_set_t *set);
```
A call to `sched_getaffinity()` retrieves the CPU affinity of the process pid and stores it in the special cpu_set_t type, which is accessed via special macros. If pid is 0, the call retrieves the current process's affinity. The setsize parameter is the size of the cpu_set_t type, which must be used by glibc for compatilibity with future changes in the size of this type.

```C
cpu_set_t set;
int ret, i;

CPU_ZERO(&set);
ret = sched_getaffinity(0, sizeof cpu_set_t, &set);
if (ret == -1)
	perror("sched_getaffinity");

for (i = 0; i < CPU_SETSIZE; ++i) {
	int cpu;
	
	cpu = CPU_ISSET(i, &set);
	printf("cpu=%i is %s\n", i,
			cpu ? "set" : "unset");
}
```

##Real-Time Systems
##Resource Limits